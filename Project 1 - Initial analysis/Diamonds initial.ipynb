{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# path = kagglehub.dataset_download(\"hrokrin/the-largest-diamond-dataset-currely-on-kaggle\")\n",
    "# filepath = path + '\\\\diamonds.csv'\n",
    "# filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "import gower\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `info_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_df(df):\n",
    "    non_null = len(df)-df.isnull().sum().values\n",
    "    null = df.isnull().sum().values\n",
    "    pct_non_null = np.round(100 * non_null / (non_null + null), 0)\n",
    "    types = df.dtypes.values\n",
    "    \n",
    "    info_df = pd.DataFrame({\n",
    "            \"name\": df.columns,\n",
    "            \"non_null\": non_null,\n",
    "            \"null\": null,\n",
    "            \"non_null_pct\": pct_non_null,\n",
    "            \"type\": types\n",
    "        })\n",
    "    \n",
    "    print(info_df)\n",
    "    print()\n",
    "    print(f'{df.shape[0]:,} rows')\n",
    "    return info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plot_corr_heatmap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_heatmap(df, method):\n",
    "    '''\n",
    "    Plots a heatmap of the correlation matrix for df.\n",
    "    Inputs:\n",
    "        Dataframe: Dataframe to compute pairwise column correlations\n",
    "        method: {‘pearson’, ‘kendall’, ‘spearman’} or callable\n",
    "    '''\n",
    "\n",
    "    # Copied code from seaborn examples\n",
    "    # https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "    sns.set(style=\"white\")\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(df.corr())\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    # sns.heatmap(df.corr(), mask=mask, cmap=cmap, vmax=1, center=0,\n",
    "    #             square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n",
    "    sns.heatmap(df.corr(method=method, numeric_only=True), cmap=cmap, vmax=1, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `print_scores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(training_score, testing_score, metric='R^2'):\n",
    "    print(f'Training {metric}: {training_score:.4}')\n",
    "    print(f'Testing {metric}: {testing_score:.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial load & inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'C:\\\\Users\\\\jlefe\\\\.cache\\\\kagglehub\\\\datasets\\\\hrokrin\\\\the-largest-diamond-dataset-currely-on-kaggle\\\\versions\\\\1\\\\diamonds.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_diamonds = pd.read_csv(filepath, index_col=0)\n",
    "raw_frame_memory = raw_diamonds.memory_usage(index=True, deep=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_diamonds_info = info_df(raw_diamonds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_diamonds.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review all categorical feature domains.  Get values to store in dictionary for CategoricalDType below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_diamonds.color.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revised load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read \"unknown\" as NaN\n",
    "2. Create & assign customized categorial data types\n",
    "2. Read `eye_clean` value of 'E1' as NaN, by excluding it from the domain specification\n",
    "    - The rating is ambiguous\n",
    "    - This rating only occurs in rows attributed to the GIA lab, whereas the GIA (Gemological Institute of America) professes not to use eye clean as a grading factor.\n",
    "    - 300 of ~220K rows - only 1.4% of data set\n",
    "3. Rename `cut` to `shape` (avoid confusion with traditional 'cut' which here is `cut_quality`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_variables = ['cut', 'lab', 'fancy_color_dominant_color', 'fancy_color_secondary_color', 'fancy_color_overtone', 'fancy_color_intensity']\n",
    "nominal_cat = {col_name:'category' for col_name in nominal_variables}\n",
    "\n",
    "# Quality categories are ordered from worst to best\n",
    "ordinal_cat = {\n",
    "    'cut_quality':CategoricalDtype(['Fair', 'Good', 'Very Good', 'Excellent', 'Ideal'], ordered=True),\n",
    "    'color':CategoricalDtype(['M', 'L', 'K', 'J', 'I', 'H', 'G', 'F', 'E', 'D'], ordered=True),\n",
    "    'clarity':CategoricalDtype(['I3', 'I2', 'I1', 'SI3', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF'], ordered=True),\n",
    "    'symmetry': CategoricalDtype(['Poor', 'Fair', 'Good', 'Very Good', 'Excellent'], ordered=True),\n",
    "    'polish': CategoricalDtype(['Poor', 'Fair', 'Good', 'Very Good', 'Excellent'], ordered=True),\n",
    "    'eye_clean': CategoricalDtype(['No', 'Borderline', 'Yes'], ordered=True),\n",
    "    'culet_size': CategoricalDtype(['EL', 'VL', 'L', 'SL', 'M', 'S', 'VS', 'N'], ordered=True),\n",
    "    'culet_condition': CategoricalDtype(['Chipped', 'Abraded', 'Pointed'], ordered=True),\n",
    "    'girdle_min': CategoricalDtype(['XTN', 'VTN', 'STN', 'TN', 'M', 'STK', 'TK', 'VTK', 'XTK'], ordered=True),\n",
    "    'girdle_max': CategoricalDtype(['XTN', 'VTN', 'STN', 'TN', 'M', 'STK', 'TK', 'VTK', 'XTK'], ordered=True),\n",
    "}\n",
    "\n",
    "ordinal_variables = list(ordinal_cat.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revised load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           name  non_null    null  non_null_pct      type\n",
      "0                         shape    219703       0         100.0  category\n",
      "1                   clear_color    210541    9162          96.0  category\n",
      "2                       clarity    219703       0         100.0  category\n",
      "3                  carat_weight    219703       0         100.0   float64\n",
      "4                   cut_quality    159096   60607          72.0  category\n",
      "5                           lab    219703       0         100.0  category\n",
      "6                      symmetry    219703       0         100.0  category\n",
      "7                        polish    219703       0         100.0  category\n",
      "8                     eye_clean     62487  157216          28.0  category\n",
      "9                    culet_size    133963   85740          61.0  category\n",
      "10              culet_condition     15319  204384           7.0  category\n",
      "11                depth_percent    219703       0         100.0   float64\n",
      "12                table_percent    219703       0         100.0   float64\n",
      "13                  meas_length    219703       0         100.0   float64\n",
      "14                   meas_width    219703       0         100.0   float64\n",
      "15                   meas_depth    219703       0         100.0   float64\n",
      "16                   girdle_min    136271   83432          62.0  category\n",
      "17                   girdle_max    135408   84295          62.0  category\n",
      "18                  fluor_color     15726  203977           7.0    object\n",
      "19              fluor_intensity     76084  143619          35.0    object\n",
      "20   fancy_color_dominant_color      9164  210539           4.0  category\n",
      "21  fancy_color_secondary_color      1062  218641           0.0  category\n",
      "22         fancy_color_overtone       388  219315           0.0  category\n",
      "23        fancy_color_intensity      9162  210541           4.0  category\n",
      "24            total_sales_price    219703       0         100.0     int64\n",
      "\n",
      "219,703 rows\n"
     ]
    }
   ],
   "source": [
    "diamonds = pd.read_csv(filepath, index_col=0, na_values=['unknown'], dtype={**ordinal_cat, **nominal_cat})\n",
    "diamonds.rename(columns={'cut':'shape', 'color':'clear_color'}, inplace=True)\n",
    "diamonds_info = info_df(diamonds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update variable lists -- they will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_variables.remove('cut')\n",
    "nominal_variables.append('shape')\n",
    "\n",
    "ordinal_variables.remove('color')\n",
    "ordinal_variables.append('clear_color')\n",
    "\n",
    "continuous_variables = list(diamonds.select_dtypes(include = 'number').columns)\n",
    "\n",
    "print(f'Continuous variables: {continuous_variables}')\n",
    "print(f'Ordinal variables: {ordinal_variables}')\n",
    "print(f'Nominal variables: {nominal_variables}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check memory improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_frame_memory = diamonds.memory_usage(index=True, deep=True).sum()\n",
    "print(f'{raw_frame_memory} -> {rev_frame_memory}')\n",
    "print(f'{rev_frame_memory/raw_frame_memory*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove `lab` column\n",
    "\n",
    "3-letter code indicating the lab that performed the analysis.  We are making an assumption that this does not have an effect on the actual measurements or assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove lab column\n",
    "diamonds.drop(columns='lab', inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there duplicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds[diamonds.duplicated()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215816, 24)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds.drop_duplicates(inplace=True)\n",
    "diamonds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode ordinal variables\n",
    "\n",
    "- **Ordered** categorical variables get label encoding, i.e., map category to an integer in order. This does not change the number of columns, only their type.\n",
    "- **Unordered** (nominal) variables get dummy-variable (one-hot) encoding, which will increase the number of columns.  Since some of these columns pertain only to fancy diamonds or only to clear diamonds, this encoding will be done later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           name  non_null    null  non_null_pct      type\n",
      "0                         shape    215816       0         100.0  category\n",
      "1                   clear_color    206717    9099          96.0  category\n",
      "2                       clarity    215816       0         100.0      int8\n",
      "3                  carat_weight    215816       0         100.0   float64\n",
      "4                   cut_quality    155573   60243          72.0   float64\n",
      "5                      symmetry    215816       0         100.0      int8\n",
      "6                        polish    215816       0         100.0      int8\n",
      "7                     eye_clean     62238  153578          29.0   float64\n",
      "8                    culet_size    132964   82852          62.0   float64\n",
      "9               culet_condition     15314  200502           7.0   float64\n",
      "10                depth_percent    215816       0         100.0   float64\n",
      "11                table_percent    215816       0         100.0   float64\n",
      "12                  meas_length    215816       0         100.0   float64\n",
      "13                   meas_width    215816       0         100.0   float64\n",
      "14                   meas_depth    215816       0         100.0   float64\n",
      "15                   girdle_min    135488   80328          63.0   float64\n",
      "16                   girdle_max    134625   81191          62.0   float64\n",
      "17                  fluor_color     15712  200104           7.0    object\n",
      "18              fluor_intensity     74982  140834          35.0    object\n",
      "19   fancy_color_dominant_color      9101  206715           4.0  category\n",
      "20  fancy_color_secondary_color      1059  214757           0.0  category\n",
      "21         fancy_color_overtone       387  215429           0.0  category\n",
      "22        fancy_color_intensity      9099  206717           4.0  category\n",
      "23            total_sales_price    215816       0         100.0     int64\n",
      "\n",
      "215,816 rows\n"
     ]
    }
   ],
   "source": [
    "# use .where(...notna()) to keep NaNs and not map them to -1\n",
    "diamonds = pd.DataFrame({\n",
    "    var_name: diamonds[var_name].cat.codes.where(diamonds[var_name].notna()) if var_name in ordinal_variables else diamonds[var_name]\n",
    "    for var_name in diamonds.columns\n",
    "})\n",
    "info_df(diamonds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 💎 Supervised regression: Can a model predict the price of a fancy (colored) diamond or a clear diamond?\n",
    "\n",
    "Because there are some columns that only apply to 'fancy' or clear diamonds, these two categories are predicted separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = diamonds.fancy_color_dominant_color.notna()\n",
    "n_fancy = colors[colors].shape[0]\n",
    "n_clear = colors[~colors].shape[0]\n",
    "print(n_fancy, n_clear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fancy diamonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_diamonds = diamonds[diamonds.fancy_color_dominant_color.notna()].copy()\n",
    "info_df(fancy_diamonds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove columns with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several columns with a high number of null values -- remove these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          name  non_null  null  non_null_pct      type\n",
      "0                        shape      9101     0         100.0  category\n",
      "1                      clarity      9101     0         100.0      int8\n",
      "2                 carat_weight      9101     0         100.0   float64\n",
      "3                     symmetry      9101     0         100.0      int8\n",
      "4                       polish      9101     0         100.0      int8\n",
      "5                depth_percent      9101     0         100.0   float64\n",
      "6                table_percent      9101     0         100.0   float64\n",
      "7                  meas_length      9101     0         100.0   float64\n",
      "8                   meas_width      9101     0         100.0   float64\n",
      "9                   meas_depth      9101     0         100.0   float64\n",
      "10  fancy_color_dominant_color      9101     0         100.0  category\n",
      "11           total_sales_price      9101     0         100.0     int64\n",
      "\n",
      "9,101 rows\n"
     ]
    }
   ],
   "source": [
    "fancy_diamonds.dropna(axis='columns', inplace=True)\n",
    "fancy_info = info_df(fancy_diamonds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `color` category gets dropped.  It is almost entirely empty, as this feature is represented by the `fancy_color` columns.\n",
    "\n",
    "Of the `fancy_color` columns, the only one with sufficient information is the `...dominant_color` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any relationship between fancy color & sales price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_diamonds.plot.scatter(x='fancy_color_dominant_color', y='total_sales_price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look for correlated features\n",
    "\n",
    "Instead of Pearson's coefficient, use Kendall's tau correlation function, which works on label-encoded ordinal variables as well as continuous variable.\n",
    "\n",
    "Kendall's tau does not work for dummy (one-hot encoded) variables, so must leave those out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_heatmap(fancy_diamonds.select_dtypes(include=\"number\"), method='kendall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The volume measurements appear strongly correlated with the weight measurement (caret).  Double-check with Pearson's correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_measures = ['meas_length', 'meas_width', 'meas_depth']\n",
    "plot_corr_heatmap(fancy_diamonds[volume_measures + ['carat_weight']], method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_diamonds.drop(columns=volume_measures, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode nominal variables with dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_nominal_variables = [col_name for col_name in fancy_diamonds.columns if col_name in nominal_variables]\n",
    "fancy_nominal_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_nominal = pd.get_dummies(fancy_diamonds[fancy_nominal_variables], dtype='int')\n",
    "fancy_ordered = fancy_diamonds.select_dtypes(include=\"number\")\n",
    "fancy_encoded = fancy_ordered.merge(fancy_nominal, right_index=True, left_index=True)\n",
    "fancy_encoded.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well can we predict the price of a colored diamond?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "- Set features & target\n",
    "- Create train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fancy = fancy_encoded.drop(columns='total_sales_price')\n",
    "y_fancy = fancy_encoded['total_sales_price']\n",
    "X_fancy_train, X_fancy_test, y_fancy_train, y_fancy_test = train_test_split(X_fancy, y_fancy, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_lr = LinearRegression()\n",
    "fancy_lr.fit(X_fancy_train, y_fancy_train)\n",
    "fancy_R2_train = fancy_lr.score(X_fancy_train, y_fancy_train)\n",
    "fancy_R2_test = fancy_lr.score(X_fancy_test, y_fancy_test)\n",
    "print_scores(fancy_R2_train, fancy_R2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try without dummy variables\n",
    "\n",
    "`cut` (cut shape) and `fancy_color`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fancy_ordered = fancy_ordered.drop(columns='total_sales_price', errors='ignore')\n",
    "X_fancy_ordered_train, X_fancy_ordered_test, y_fancy_ordered_train, y_fancy_ordered_test = train_test_split(X_fancy_ordered, y_fancy, random_state=42, shuffle=True)\n",
    "\n",
    "fancy_ordered_lr = LinearRegression()\n",
    "fancy_ordered_lr.fit(X_fancy_ordered_train, y_fancy_ordered_train)\n",
    "fancy_ordered_R2_train = fancy_ordered_lr.score(X_fancy_ordered_train, y_fancy_ordered_train)\n",
    "fancy_ordered_R2_test = fancy_ordered_lr.score(X_fancy_ordered_test, y_fancy_ordered_test)\n",
    "print_scores(fancy_ordered_R2_train, fancy_ordered_R2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear diamonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_diamonds = diamonds[diamonds.fancy_color_dominant_color.isna()].drop(columns=[col_name for col_name in diamonds.columns if col_name.startswith('fancy')])\n",
    "clear_info = info_df(clear_diamonds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove columns with > 50% null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eye_clean', 'culet_condition', 'fluor_color', 'fluor_intensity']\n"
     ]
    }
   ],
   "source": [
    "drop_cols = list(clear_info[clear_info.non_null_pct < 50]['name'])\n",
    "print(drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_diamonds.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "clear_info = info_df(clear_diamonds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the rest of the missing data by removing rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_diamonds.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look for correlations\n",
    "\n",
    "Exclude the one nominal variable, `shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_heatmap(clear_diamonds.drop(columns=['shape', 'total_sales_price']), method='kendall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller heatmap for slide deck\n",
    "sns.heatmap(clear_diamonds.drop(columns='total_sales_price').corr(numeric_only=True, method='kendall'), cmap=sns.diverging_palette(220, 10, as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove correlated variables\n",
    "Again, the volume measurements are strongly correlated with the weight measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_diamonds.drop(columns=volume_measures, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the one nominal variable, `shape`\n",
    "\n",
    "This identifies the cut shape, not the cut quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clear_diamonds['shape'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_nominal = pd.get_dummies(clear_diamonds['shape'], dtype='int')\n",
    "clear_ordered = clear_diamonds.drop(columns='shape', errors='ignore')\n",
    "clear_encoded = clear_ordered.merge(clear_nominal, right_index=True, left_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "- Set features & target\n",
    "- Create train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clear = clear_encoded.drop(columns='total_sales_price', errors='ignore')\n",
    "y_clear = clear_encoded['total_sales_price']\n",
    "X_clear_train, X_clear_test, y_clear_train, y_clear_test = train_test_split(X_clear, y_clear, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_lr = LinearRegression()\n",
    "clear_lr.fit(X_clear_train, y_clear_train)\n",
    "R2_clear_train = clear_lr.score(X_clear_train, y_clear_train)\n",
    "R2_clear_test = clear_lr.score(X_clear_test, y_clear_test)\n",
    "print_scores(R2_clear_train, R2_clear_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 💎 Supervised Classification:\n",
    "# Can a model distinguish colored diamonds from clear diamonds even if the color columns are missing?\n",
    "- Combine `fancy` and `clear` data sets\n",
    "- Replace the color column with Boolean `is_fancy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In `fancy_diamonds` rename `fancy_color_dominant_color` to `dominant_color`\n",
    "- In `clear_diamonds` add `dominant_color` column with value 'clear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_diamonds.rename(columns={'fancy_color_dominant_color':'dominant_color'}, inplace=True)\n",
    "clear_diamonds['dominant_color'] = 'clear'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine `fancy` and `clear` into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shape',\n",
       " 'table_percent',\n",
       " 'polish',\n",
       " 'carat_weight',\n",
       " 'clarity',\n",
       " 'depth_percent',\n",
       " 'symmetry',\n",
       " 'total_sales_price',\n",
       " 'dominant_color']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which columns are useful for both sets?\n",
    "clear_columns = set(clear_diamonds.columns)\n",
    "fancy_columns = set(fancy_diamonds.columns)\n",
    "common_columns = list(clear_columns.intersection(fancy_columns))\n",
    "common_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_diamonds = pd.concat([clear_diamonds[common_columns], fancy_diamonds[common_columns]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the cut column again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_diamonds_no_shape = all_diamonds.drop(columns='shape', errors='ignore')\n",
    "shape_dummies = pd.get_dummies(all_diamonds['shape'], dtype='int', prefix='shape')\n",
    "all_diamonds = all_diamonds_no_shape.merge(shape_dummies, right_index=True, left_index=True)\n",
    "all_diamonds.columns = all_diamonds.columns.str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_diamonds.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Boolean `is_fancy` and remove `dominant_color` for this exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_diamonds['is_fancy'] = all_diamonds['dominant_color'] != 'clear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_color = all_diamonds.drop(columns='dominant_color')\n",
    "no_color.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proportion of fancy diamonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_pct = no_color.is_fancy.value_counts()[True] / no_color.is_fancy.value_counts()[False]\n",
    "print(f'{fancy_pct:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Balanced Random Forest classifier\n",
    "\n",
    "Use a balanced model because the proportion of fancy diamonds is quite small (~10%)\n",
    "\n",
    "Notes on the parameters from the documentation: The default of `bootstrap` will change from `True` to `False` in version 0.13. **Bootstrapping is already taken care by the internal sampler using `replacement=True`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "- Set features & target\n",
    "- Create train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_no_color = no_color.drop(columns='is_fancy')\n",
    "y_all_no_color = no_color['is_fancy']\n",
    "X_all_no_color_train, X_all_no_color_test, y_all_no_color_train, y_all_no_color_test = train_test_split(X_all_no_color, y_all_no_color, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_rfc = BalancedRandomForestClassifier(sampling_strategy='all', replacement=True, bootstrap=False)\n",
    "balanced_rfc.fit(X_all_no_color_train, y_all_no_color_train)\n",
    "y_all_no_color_train_pred = balanced_rfc.predict(X_all_no_color_train)\n",
    "y_all_no_color_test_pred = balanced_rfc.predict(X_all_no_color_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_all_no_color_train, y_all_no_color_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy: {accuracy_score(y_all_no_color_train, y_all_no_color_train_pred):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_all_no_color_train, y_all_no_color_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_all_no_color_train, y_all_no_color_train_pred, pos_label=1)\n",
    "\n",
    "# Find auc\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot of a ROC curve for class 1 (has_cancer)\n",
    "plt.figure(figsize=[8,8])\n",
    "\n",
    "# Plot fpr, tpr\n",
    "plt.plot(fpr, tpr, color='skyblue', lw = 2, label = 'ROC curve (area = %0.2f)' % roc_auc)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.title('ROC for clear/fancy detection (training)', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_all_no_color_test, y_all_no_color_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy: {accuracy_score(y_all_no_color_test, y_all_no_color_test_pred):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_all_no_color_test, y_all_no_color_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_all_no_color_test, y_all_no_color_test_pred, pos_label=1)\n",
    "\n",
    "# Find auc\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot of a ROC curve for class 1 (has_cancer)\n",
    "plt.figure(figsize=[8,8])\n",
    "\n",
    "# Plot fpr, tpr\n",
    "plt.plot(fpr, tpr, color='skyblue', lw = 2, label = 'ROC curve (area = %0.2f)' % roc_auc)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.title('ROC for clear/fancy detection (test)', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 💎 Unsupervised classification: DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the `is_fancy` column from the last exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_diamonds.drop(columns='is_fancy', inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_diamonds.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode `dominant_color`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = pd.get_dummies(all_diamonds['dominant_color'], dtype='int', prefix='color')\n",
    "all_diamonds.drop(columns='dominant_color', inplace=True, errors='ignore')\n",
    "all_diamonds = all_diamonds.merge(colors, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_diamonds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "DBSCAN relies on distance measurements to find clusters.  Because there are mixed dtypes in the data set, we will use Gower's distance as the distance metric.  Unlike Euclidean distance, Gower's, a rank-based metric, can handle mixed data types.\n",
    "\n",
    "Ordinarily we would first scale the data. However Gower's scales the data internally so it is not necessary to perform a separate scaling step.\n",
    "\n",
    "Gower's distance is not implemented in scikit-learn, so we have to compute the distance matrix separately and pass that to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🛑✋🏼 STOP ✋🏼🛑\n",
    "# Do not run this cell.  Calculating the matrix takes ~ 40 minutes.\n",
    "#\n",
    "# It is stored as a pickle.  Retrieve it below. The code is commented out for protection.\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "distance_matrix = gower.gower_matrix(all_diamonds)\n",
    "\n",
    "with open('gower_matrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(distance_matrix, handle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gower_matrix.pickle', 'rb') as handle:\n",
    "    distance_matrix = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose parameters\n",
    "\n",
    "Rule of thumb for number of neighbors is 2 x number of features.\n",
    "\n",
    "Then use an elbow curve to determine the right value for epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of neighbors: k is usually the same as 'min_samples' in DBSCAN\n",
    "k = min_samples # k = min_samples = 2 * # of features = 2 * 7\n",
    "\n",
    "neighbors = NearestNeighbors(n_neighbors=k)\n",
    "neighbors_fit = neighbors.fit(all_diamonds)\n",
    "distances, indices = neighbors_fit.kneighbors(all_diamonds)\n",
    "\n",
    "# Sort the distances of the k-th nearest neighbor\n",
    "distances = np.sort(distances[:, k-1])\n",
    "\n",
    "# Plot the k-distance graph (Elbow Curve)\n",
    "# ymax = 5\n",
    "# gridunit = 0.25\n",
    "\n",
    "plt.plot(distances)\n",
    "# plt.ylim(ymax=ymax)\n",
    "# plt.yticks(np.arange(0, ymax)) # gridunit\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(f\"{k}-th Nearest Neighbor Distance\")\n",
    "plt.title(f\"{k}-th Nearest Neighbor Distance Elbow Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "epsilon = 0.25\n",
    "db = DBSCAN(eps=epsilon, min_samples=min_samples, metric = \"precomputed\").fit(distance_matrix)\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "labels = db.labels_\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = list(labels).count(-1)\n",
    "\n",
    "print(f\"Estimated number of clusters: {n_clusters}\")\n",
    "print(f\"Estimated number of noise points: {n_noise}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "all_diamonds_pca = pd.DataFrame(pca.fit_transform(all_diamonds), columns=['PCA_1', 'PCA_2'])\n",
    "all_diamonds_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance=np.var(all_diamonds_pca,axis=0)\n",
    "explained_variance_ratio = explained_variance/np.sum(explained_variance)\n",
    "print(f'total explained variance: {explained_variance_ratio.sum()}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance_ratio.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(all_diamonds_pca, x='PCA_1', y='PCA_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pca_distance_matrix = gower.gower_matrix(all_diamonds_pca)\n",
    "\n",
    "with open('pca_gower_matrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(pca_distance_matrix, handle)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pca_gower_matrix.pickle', 'rb') as handle:\n",
    "    pca_distance_matrix = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🛑✋🏼 Caution - this is likely to crash ✋🏼🛑\n",
    "epsilon = 0.25\n",
    "db = DBSCAN(eps=epsilon, min_samples=min_samples, metric = \"precomputed\").fit(pca_distance_matrix)\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "labels = db.labels_\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise = list(labels).count(-1)\n",
    "\n",
    "print(f\"Estimated number of clusters: {n_clusters}\")\n",
    "print(f\"Estimated number of noise points: {n_noise}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IOD2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
